

\section{Conclusion}

In this paper, we propose a modified version of the Decision Transformer for robotic imitation learning and evaluate its performance on sub-optimal datasets. We show that the Transformer's ability to understand context and long-term dependencies can improve action predictions and outperform return-conditioned behavioral cloning. However, we also find that conditioning on a history of previous actions and RTG scalars can hinder the model's performance and result in distributional shifts during evaluation. We demonstrate that increasing model size can improve performance on both tasks and that longer context sequence lengths can improve prediction accuracy during training. When compared to simpler baselines, we see clear improved performance on both tasks.

Our work has the potential to be applied in various robotics tasks, particularly those that involve working with sub-optimal or ``noisy" data. General robotics research focuses on designing robots that can operate in unstructured environments, such as homes and offices, where data collection for manipulation tasks can be noisy and difficult. Our approach could potentially be a useful starting point for using Decision Transformers to improve the performance of robots in these types of settings. For instance, we can use past versions of data to improve our models over time, employ a wide range of policies on different data types, and reduce the need for extensive data cleaning before training. Additionally, future work should focus on mitigating the distributional shifts observed in our approach and exploring ways of improving the reward function to better support dense rewards. We open-source the entire project as a stepping stone in these endeavors. 
