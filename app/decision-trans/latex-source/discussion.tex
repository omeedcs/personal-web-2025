\section{Discussion and Future Work}
\label{sec:discussion}
Our experiments suggest that return-conditioned imitation learning methods like Decision Transformers can outperform standard Behavioral Cloning when learning from mixed-quality data. However, our implementation of DT still underperforms BC models that use the highest quality subset of the dataset (PH). The real promise of Decision Transformer is its ability to mimic high-quality data while still learning whatever may be useful from a large quantity of sub-optimal demonstrations. Ideally, DT would strictly outperform the BC-PH baselines and make use of MG datasets in more difficult robomimic tasks like Square and Tool Hang. We offer three possible explanations for why we are not seeing these results, and suggest directions for future improvement.

First, limited computational resources have restricted our ability to perform large hyperparameter sweeps. It is possible that there are higher-performance model architectures, regularization settings, and other empirical details that could improve our results. We hope that our open-source code release will be a useful starting point for future experiments.

Second, there may be alternative reward functions that are more suitable for return-conditioned BC than the time-based completion bonus used in our experiments (Eq. \ref{eq:rt}). Rewarding the agent for the time to completion clearly separates poor MG demonstrations from quality PH ones. However, it may over-emphasize fast solutions instead of consistent and easily repeatable behavior. Furthermore, the time-based rewards may be too difficult for the sequence model to imitate precisely because it is difficult to identify the current time with a fixed-length context window. Results such as Fig. \ref{fig:success_rate_curve} show that current policies struggle to imitate low-return (slow) demonstrations. We plan to add a timestep counter to the state representation in the open-source release and re-evaluate on the more difficult robomimic tasks.

Finally, there may be meaningful algorithmic changes needed to address some limitations of return-conditioned imitation learning in the robomimic setting. Because the robomimic datasets contain much more machine-generated data than human demonstrations, our return-to-go labeled dataset is heavily skewed toward low-return outcomes. Relabeling trajectories in hindsight with their achieved return creates an implicit ``task-averaging" or return-agnostic term in the resulting policy, which can be especially problematic in cases where the outcome distribution is biased. For more on this issue, we refer the reader to \citep{eysenbach2022imitating}.
