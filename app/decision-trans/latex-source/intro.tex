\section{Introduction}
% Recent work has reformulated offline reinforcement learning as a sequence modeling problem that can be effectively solved with large Transformer models. Inspired by previous work on decision transformers, this paper re-implements Decision Transformer \cite{decisiontransformer} for use in the robomimic \cite{robomimic2021} benchmark. Transformers have taken over many subfields in ML, including Natural Language Processing, Natural Language Generation, and Computer Vision. We are motivated by the success of GPT-x, BERT, and other large language models outside of robotics. We evaluate our trained policies on two simulated manipulation tasks in robomimic \cite{robomimic2021} using low-dimensional states instead of image observations to reduce computational requirements. Our data source is a subset of the datasets included in the robomimic benchmark that includes large machine-generated data. Each task has multiple offline datasets based on quality; our focus is to have a decision transformer learn from mixed settings containing both novice and expert demonstrations, as this is where Decision Transformer's return-conditioning should provide the biggest advantage by filtering out failed attempts. We implement a custom version of the decision transformer model for robomimic data focused on continuous action spaces. We experiment with a simpler input embedding that saves compute by shortening the input sequence. We evaluate and analyze the average return of the policy in the test environment. We evaluate the policy's ability to replicate multiple levels of desired return. We measure the average success rate of $50$ evaluation roll-outs. Our model outperforms behavioral cloning accuracy on the Lift and Can tasks trained on mixed-quality data. All of our transformer code and experiments are available through GitHub.



Robot imitation learning methods use supervised learning to mimic the actions necessary to complete complex tasks. This process typically involves a time-consuming cycle of collecting and filtering high-quality human demonstrations. In order to increase data availability and ease of use, we would like to learn from sub-optimal demonstrations. Sources of sub-optimal data may include inexperienced human teleoperators, scripted agents, or past versions of our own learning-based policy. Learning from fixed datasets of mixed-quality experience is often studied in offline Reinforcement Learning (RL) \cite{levine2020offline}. Despite a recent surge in research interest, offline RL remains challenging in practice, and it can be difficult to develop a method that achieves real-world results. Our project focuses on the Decision Transformer \cite{decisiontransformer} - a simple and powerful alternative to RL algorithms that replaces brittle dynamic programming and policy gradients with standard sequence modeling. Like more traditional offline RL methods, Decision Transformer (DT) can learn from mixed-quality demonstrations by conditioning action outputs on the total return of each trajectory. DT policies can then imitate high-quality data while gathering extra information about the task by modeling low-quality demonstrations. At test-time, we can specify an expert-level target return and deploy a policy with the performance of expert imitation learning without the need for expensive curated datasets. DT reduces offline RL to a training routine that resembles sequence models in supervised learning. There is optimism that we can leverage diverse datasets and large network architectures similar to those that have been successful in domains like natural language processing to improve generalization in robot decision-making. 

    \par In this project, we implement a custom version of the Decision Transformer focusing on continuous action spaces and stochastic policies. We evaluate our implementations' ability to learn from mixed-quality data using two simulated domains from the robomimic benchmark \cite{robomimic2021} that provide machine-generated and expert demonstrations. We study the impact of several design decisions on task success rates, including model size, policy parameterization, and context sequence lengths. Results demonstrate that our DT implementation significantly outperforms naive Behavioral Cloning in this mixed-quality and multi-modal setting. We also discuss some limitations of our work and opportunities for future improvement on the robomimic benchmark. The code for this project is open-sourced and available on GitHub to enable future research\footnote{\url{https://github.com/jakegrigsby/robomimic-decision-transformer}}.


