
\section{Background and Related Work}
\label{sec:relatedwork}

\subsection{Reinforcement Learning}
In the Reinforcement Learning  setting considered in this paper, an agent interacts with an environment in order to maximize a reward signal. At each timestep $t$, the agent perceives a state $s_t$ and selects an action $a_t$, leading to a new state $s_{t+1}$ and a reward $r_t$. The agent's decision-making strategy is defined by its policy $\pi$, which maps states to a distribution over actions $\pi(a_t \mid s_t)$. Our goal at every timestep is to maximize the cumulative sum of rewards until some time limit of $H$ steps, also known as the return. We will use the term ``return-to-go" (RTG) to refer to the return after a specific timestep $t$. The RTG at timestep $t$ is denoted $\hat{R}_t = \mathop{\sum}_{i=t}^{H}r_i$. Decision Transformer computes RTG values in hindsight from an offline dataset and uses them to differentiate between low and high-quality demonstrations.

\subsection{Offline Reinforcement Learning}
Offline RL \cite{levine2020offline} studies decision-making from fixed datasets of prior experience. Unlike standard ``online" RL, offline agents cannot explore the environment to gather new information about the task. Instead, offline methods attempt to stitch together optimal decision-making strategies from datasets of sub-optimal data to match or exceed the performance of the policies that generated the training dataset. Offline algorithms often have to confront technical challenges 
arising from situations our dataset does not cover. For example, methods such as CQL \cite{kumar2020conservative}, CRR \cite{NEURIPS2020_588cb956}, and BCQ \cite{fujimoto2019off} restrict their policies' action choices to those present in the dataset in order to avoid out-of-distribution optimization issues. Model-based methods such as MOPO \cite{yu2020mopo} explicitly penalize actions that lead to uncertain states. The ultimate goal of offline RL is to develop a scalable way to reuse data and deploy RL to real-world tasks, where data collection is expensive or unsafe. Examples include self-driving cars, healthcare, and real-world robotics \cite{levine2020offline}. However, current methods can be difficult to develop and tune without the ability to evaluate policies in the test environment. Recent research has worked to simplify techniques \cite{fujimoto2021minimalist} and develop a repeatable workflow \cite{kumar2021workflow} for new applications.


\subsection{Robomimic}
Robomimic\footnote{\url{https://github.com/ARISE-Initiative/robomimic}} \cite{robomimic2021} is a framework for offline robot learning from demonstrations as a part of the Advancing Robot Intelligence through Simulated Environments (ARISE) Initiative. Robomimic includes a unique collection of standardized human-collected and machine-generated datasets for offline RL and imitation learning. The benchmark currently supports several simulated manipulation tasks including ``Lift", ``Can", ``Tool Hang", and ``Square Transport". These tasks use the robosuite simulator \cite{zhu2020robosuite} to enable affordable access to offline robotics research. These simulated environments can be configured to use low-dimensional proprioceptive states instead of high-dimensional rendered images. Robomimic also includes real-robot variants of similar tasks with camera-based observations.

Robomimic divides each task's offline dataset into several categories based on demonstration quality and extensively benchmarks behavioral cloning (BC) and offline RL algorithms. The results suggest that learning from a mixture of human datasets can be challenging because of the varying proficiency and strategies of multiple demonstrators. Each teleoperator solves the same task slightly differently, which leads to multi-modality even in simple tasks; this effect would likely be more problematic in the kinds of long-horizon tasks we would eventually like robots to solve. The robomimic authors find that offline RL methods can be challenging to use in practice and are often outperformed by simpler BC techniques. In particular, they find that the strongest baseline is a memory-equipped BC policy that uses a Recurrent Neural Network (RNN) to process sequences of observations. This BC-RNN baseline improves action prediction by using its memory of past timesteps to address the multi-modality of mixed human demonstrators. However, BC-RNN still struggles when the dataset contains low-quality demonstrations because it learns to imitate all of the decisions in the training set as one unified policy.
% The eight tasks have been extensively tested on 6 offline learning algorithms BC, BC-RNN, HBC, BCQ, CQL, IRIS. The tasks have also been tested in 2 observation spaces: 1) low-dimensional agents with ground truth and 2) image agents that receive camera observations. Information and the code repository for robomimic are available through GitHub\footnote{\url{https://github.com/ARISE-Initiative/robomimic}}. 

% More formally known as "Behavioral Cloning with Recurrent Neural Network policy", BC-RNN enables us to model temporal correlations in decision-making \cite{robomimic2021}. In one of the core lessons from robomimic, it was found that methods that make decisions based on history, such as BC-RNN, tend to outperform other methods, especially on human datasets. \cite{robomimic2021}. RNNs, however, are computationally intensive due to their un-parallelizable architecture, and they are susceptible to the exploding gradient problem. Additionally, they are not as good as transformers at handling long-term dependencies. Transformers, on the other hand, have recently been found to generate higher-quality output than RNN sequence-to-sequence models while demanding less computation \cite{transformer}. These issues with RNNs are some of the leading motivations for us to use transformer architecture.

% The variability of human behavior presents a significant challenge to imitation learning algorithms, like BC-RNN, attempting to learn from mixed-quality data. Imitation learning algorithms struggle when trained from multiple human demonstrators, as the algorithms must be able to accurately capture and generalize from the diverse ways in which different individuals may perform a given task. Furthermore, the varying goals and objectives of these individuals can make it difficult for the algorithm to determine the appropriate action to take in a given situation. This can lead to suboptimal performance, particularly when the presence of multiple demonstrators makes it difficult for the algorithm to identify the most important or relevant actions to learn from.


\subsection{Outcome-Conditioned Behavioral Cloning}
Recent work has looked to simplify offline RL by creating similarities to supervised learning techniques in related fields like natural language processing and computer vision. One promising approach adds the return-to-go of the offline trajectory as an extra input to a BC algorithm: $\pi(a_t \mid s_t, \hat{R}_t)$. This lets the policy differentiate between high and low-quality experiences during training. Low-quality data helps increase the size of the training set and may provide some valuable knowledge of the task. However, we can replicate only the high-quality actions at test-time by specifying an expert-level target return $\hat{R}_0$. Return-conditioned behavior cloning can often match or outperform more complicated RL algorithms \cite{rvs, rcp, brandfonbrener2022does}. The idea of conditioning behavioral cloning on the quality of the demonstration datasets also appears as ``Upside Down RL" \cite{udrl_idea, udrl_implementation}. The method has conceptual similarities to goal-conditioned reinforcement learning \cite{nair2018visual, andrychowicz2017hindsight}, which we can reformulate as a supervised learning problem conditioned on states instead of returns \cite{eysenbach2022imitating, ghosh2019learning}.

\subsection{Decision Transformers}
The Transformer architecture \cite{transformer} uses an attention mechanism to process sequences without convolution or recurrence. Originally utilized for machine translation in natural language processing, the Transformer architecture has since appeared in state-of-the-art models for tasks across nearly every domain in machine learning. Transformers rely on a scaled dot-product attention mechanism that enables the representations of each timestep to interpret patterns across all the preceding timesteps in the sequence. Transformers are highly compatible with GPU parallelization and can learn complex temporal dependencies. Transformers are generally known to outperform RNN models while being more computationally efficient.

Decision Transformer \cite{decisiontransformer} is an algorithm that combines the simplicity of return-conditioned BC with the performance of large Transformer networks. The Trajectory Transformer \cite{trajectorytransformer} is a concurrent work that adds language modeling techniques like beam search to RL. Additional extensions of the Decision Transformer concept include online fine-tuning \cite{online_decisiontransformer}, few-shot prompting \cite{xu2022prompting}, and applications to stochastic environments \cite{esper}. Finally, Gato \cite{gato} uses a similar architecture and tokenization scheme to Decision Transformer, although on a much larger scale and without return-conditioning.